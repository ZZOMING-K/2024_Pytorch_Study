{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43c0417",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "###  필요한 이유 \n",
    "* 신경망의 내부 가중치 파라미터를 잘 조절하여 **원하는 함수를 근사계산하고 싶을 때 활용** \n",
    "* 손실함수를 통해 **더 좋은 가중치 파라미터 선택 가능** \n",
    "* 손실 값을 **`가중치 파라미터로 미분`** 하여 **`그래디언트 벡터`** 를 구하고 ,\n",
    "* **반대 방향으로 가중치 파라미터를 업데이트**하면서 점진적으로 더 낮은 손실 값을 갖는 가중치 파라미터를 구할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba411de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "912ad49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1,.2,.3],\n",
    "                           [.4,.5,.6],\n",
    "                           [.7,.8,.9]]) \n",
    "x = torch.rand_like(target)\n",
    "x.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb55ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0515, 0.9451, 0.0815],\n",
      "        [0.7376, 0.7345, 0.3697],\n",
      "        [0.9189, 0.5949, 0.6095]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d6eff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d049cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1113, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d56c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss : 6.731786e-02\n",
      "tensor([[0.0623, 0.7795, 0.1301],\n",
      "        [0.6626, 0.6824, 0.4209],\n",
      "        [0.8702, 0.6405, 0.6741]], requires_grad=True)\n",
      "2-th Loss : 4.072314e-02\n",
      "tensor([[0.0707, 0.6507, 0.1678],\n",
      "        [0.6042, 0.6419, 0.4607],\n",
      "        [0.8324, 0.6759, 0.7243]], requires_grad=True)\n",
      "3-th Loss : 2.463499e-02\n",
      "tensor([[0.0772, 0.5506, 0.1972],\n",
      "        [0.5588, 0.6103, 0.4917],\n",
      "        [0.8030, 0.7035, 0.7633]], requires_grad=True)\n",
      "4-th Loss : 1.490265e-02\n",
      "tensor([[0.0823, 0.4727, 0.2200],\n",
      "        [0.5235, 0.5858, 0.5157],\n",
      "        [0.7801, 0.7250, 0.7937]], requires_grad=True)\n",
      "5-th Loss : 9.015181e-03\n",
      "tensor([[0.0862, 0.4121, 0.2378],\n",
      "        [0.4961, 0.5668, 0.5345],\n",
      "        [0.7623, 0.7416, 0.8173]], requires_grad=True)\n",
      "6-th Loss : 5.453628e-03\n",
      "tensor([[0.0893, 0.3649, 0.2516],\n",
      "        [0.4747, 0.5519, 0.5490],\n",
      "        [0.7485, 0.7546, 0.8357]], requires_grad=True)\n",
      "7-th Loss : 3.299108e-03\n",
      "tensor([[0.0916, 0.3283, 0.2624],\n",
      "        [0.4581, 0.5404, 0.5603],\n",
      "        [0.7377, 0.7647, 0.8500]], requires_grad=True)\n",
      "8-th Loss : 1.995756e-03\n",
      "tensor([[0.0935, 0.2998, 0.2707],\n",
      "        [0.4452, 0.5314, 0.5692],\n",
      "        [0.7293, 0.7725, 0.8611]], requires_grad=True)\n",
      "9-th Loss : 1.207309e-03\n",
      "tensor([[0.0949, 0.2776, 0.2772],\n",
      "        [0.4352, 0.5244, 0.5760],\n",
      "        [0.7228, 0.7786, 0.8697]], requires_grad=True)\n",
      "10-th Loss : 7.303472e-04\n",
      "tensor([[0.0961, 0.2604, 0.2823],\n",
      "        [0.4273, 0.5190, 0.5813],\n",
      "        [0.7177, 0.7834, 0.8765]], requires_grad=True)\n",
      "11-th Loss : 4.418148e-04\n",
      "tensor([[0.0969, 0.2469, 0.2862],\n",
      "        [0.4213, 0.5148, 0.5855],\n",
      "        [0.7138, 0.7871, 0.8817]], requires_grad=True)\n",
      "12-th Loss : 2.672706e-04\n",
      "tensor([[0.0976, 0.2365, 0.2893],\n",
      "        [0.4165, 0.5115, 0.5887],\n",
      "        [0.7107, 0.7899, 0.8858]], requires_grad=True)\n",
      "13-th Loss : 1.616822e-04\n",
      "tensor([[0.0982, 0.2284, 0.2917],\n",
      "        [0.4129, 0.5089, 0.5912],\n",
      "        [0.7083, 0.7922, 0.8889]], requires_grad=True)\n",
      "14-th Loss : 9.780780e-05\n",
      "tensor([[0.0986, 0.2221, 0.2935],\n",
      "        [0.4100, 0.5070, 0.5932],\n",
      "        [0.7065, 0.7939, 0.8914]], requires_grad=True)\n",
      "15-th Loss : 5.916764e-05\n",
      "tensor([[0.0989, 0.2172, 0.2950],\n",
      "        [0.4078, 0.5054, 0.5947],\n",
      "        [0.7050, 0.7953, 0.8933]], requires_grad=True)\n",
      "16-th Loss : 3.579274e-05\n",
      "tensor([[0.0991, 0.2134, 0.2961],\n",
      "        [0.4061, 0.5042, 0.5959],\n",
      "        [0.7039, 0.7963, 0.8948]], requires_grad=True)\n",
      "17-th Loss : 2.165238e-05\n",
      "tensor([[0.0993, 0.2104, 0.2970],\n",
      "        [0.4047, 0.5033, 0.5968],\n",
      "        [0.7031, 0.7971, 0.8959]], requires_grad=True)\n",
      "18-th Loss : 1.309832e-05\n",
      "tensor([[0.0995, 0.2081, 0.2976],\n",
      "        [0.4037, 0.5025, 0.5975],\n",
      "        [0.7024, 0.7978, 0.8968]], requires_grad=True)\n",
      "19-th Loss : 7.923712e-06\n",
      "tensor([[0.0996, 0.2063, 0.2982],\n",
      "        [0.4028, 0.5020, 0.5981],\n",
      "        [0.7018, 0.7983, 0.8975]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#while 반복문을 사용해 두 텐서 값의 차이가 threshold 의 값보다 작아질 때 까지 반복수행 \n",
    "\n",
    "threshold = 1e-5\n",
    "learning_rate = 1\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold :\n",
    "    iter_cnt += 1\n",
    "    loss.backward() \n",
    "    \n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    x.detach_()\n",
    "    x.requires_grad_(True) \n",
    "    \n",
    "    loss =F.mse_loss(x , target) \n",
    "    print('%d-th Loss : %4e' % (iter_cnt , loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "412167c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.9237e-06, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fc737",
   "metadata": {},
   "source": [
    "# Pytoch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "918791ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef6b1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                      [3,4]]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154d79db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = x+2\n",
    "x2 = x-2\n",
    "x3 = x1*x2 \n",
    "y = x3.sum()\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8b019",
   "metadata": {},
   "source": [
    "* 생성된 결과 텐서들이 모두 grad_fn 속성을 갖는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b86d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() #이때 y는 scala여야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abe85969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8290901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60fd51ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x3\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5beefe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  0.],\n",
       "       [ 5., 12.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.detach_().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e50e18",
   "metadata": {},
   "source": [
    "# Why we gradient descent ? \n",
    "\n",
    "1. 원하는 출력값을 출력하는 함수를 근사하고 싶다.\n",
    "2. 손실함수를 최소화하도록 모델의 파라미터를 조절하자.\n",
    "3. 미분을 통해 `gradient`를 얻고, `loss 를 낮추는 방향`으로 파라미터 업데이트 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
